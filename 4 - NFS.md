# Sum√°rio

[toc]

[Introdu√ß√£o](#Introdu√ß√£o)
[NFS sobre TCP e UDP](#NFS-sobre-TCP-e-UDP)
[Remote Procedure Calls - RPC](#Remote-Procedure-Calls---RPC)
[State - Mudan√ßa da vers√£o](#State---Mudan√ßa-da-vers√£o)
[Exports - Mudan√ßa da vers√£o](#Exports---Mudan√ßa-da-vers√£o)
[Seguran√ßa](#Seguran√ßa)
[Desempenho na nova vers√£o](#Desempenho-na-nova-vers√£o)
[Funcionamento do NFS](#Funcionamento-do-NFS)
    [Comportamento notado](#Comportamento-notado)
[Arquivo exports](#Arquivo-exports)
[Daemons relacionados ao NFS](#Daemons-relacionados-ao-NFS)
    [RPCBIND](#RPCBIND)
    [Mountd](#Mountd)
    [NFSD](#NFSD)
[Acesso root e nobody](#Acesso-root-e-nobody)
[Resumo](#Resumo)
[Comandos](#Comandos)
    [exportfs](#exportfs)
    [showmount](#showmount)
    [rpcinfo](#rpcinfo)
    [NFSSTAT](#NFSSTAT)
[Melhorando o desempenho](#Melhorando-o-desempenho)



## **Introdu√ß√£o**

O NFS foi desenvolvido pela Sun Microsystem com o prop√≥sito de compartilhar diret√≥rios na rede local. Seu funcionamento √© cliente/servidor, tendo pacotes separados para cada um, um conjunto de pacotes para o Servidor e outro para o Cliente.



Vamos a uma r√°pida introdu√ß√£o dos pacotes:

| Pacotes           | Distro      | Descri√ß√£o                                     |
| ----------------- | ----------- | --------------------------------------------- |
| nfs-common        | Debian like | Utilit√°rio para o Cliente NFS.                |
| nfs-kernel-server | Debian like | Pacote para o Servidor NFS.                   |
| nfs-utils         | RedHat like | Utilit√°rio para o Cliente NFS e Servidor NFS. |



## NFS sobre TCP e UDP

Inicialmente o NFS foi desenvolvido para trabalhar em cima do UDP, porque ele tinha o melhor desempenho nas Redes e nos Servidores daquela √©poca (estamos falando do NFS vers√£o 2), o NFS por esse motivo (trabalhar com UDP) fazia remontagem da sequ√™ncia dos pacotes e verifica√ß√£o de erros (j√° que o UDP n√£o faz), mas ainda fica de fora algo muito importante, controle de congestionamento, √© essencial para o bom desempenho de redes IP de grande porte (nem UDP nem NFS fazem isso). Na vers√£o 3 do NFS, podemos escolher entre TCP ou UDP, e na vers√£o 4 do NFS, temos somente o uso do TCP.

O NFS vers√£o 4 requer TCP como protocolo padr√£o de transporte e se comunica na porta 2049. J√° as vers√µes anteriores podem ter sua porta modificada, assim como o protocolo de transporte. 

Por padr√£o, o NFS sempre vai preferir o TCP, mas para vers√£o 2 e 3, onde temos o uso do rpcbind (antigo *portmap*), a comunica√ß√£o com o *rpcbind* e com o *mountd* √© feita infelizmente usando UDP.

Rpcbind √© realizado na porta 111 e mountd uma porta aleat√≥ria por padr√£o mas voc√™ pode mudar ela, vou descrever isso em mais detalhes no decorrer do documento.



## Remote Procedure Calls - RPC

Durante o desenvolvimento do NFS, a equipe de desenvolvimento percebeu que muitos dos problemas relacionados √† rede no projeto, que precisavam ser resolvidos para que o NFS funcionasse se aplicavam tamb√©m √† outros servi√ßos baseados em rede. Com isso em mente, eles desenvolveram um m√©todo para *chamadas de procedimento remoto* conhecido como RPC. Dessa forma eles resolveriam o problema do NFS e de muitos outros servi√ßos baseados em rede, o RPC abriu as portas para que aplicativos de todos os tipos pudessem executar procedimentos em sistemas remotos como se estivessem sendo executados localmente.

As opera√ß√µes que leem/gravam arquivos, montam sistemas de arquivos, acessam metadados de arquivos e verificam permiss√µes de arquivos s√£o implementadas como RPCs.

Alguns exemplos encontrados s√£o:

- rpc.mountd

  Usado para montar o compartilhamento.

- rpcbind (antigo *portmap*)

  Usado para fazer o bind entre as portas do NFS server e Mountd, para que o cliente possa se comunicar com os protocolos corretos nos momentos certos.

- rpc.gssd

  Usado para implementar uma camada de seguran√ßa centralizada.





## State - Mudan√ßa da vers√£o

As vers√µes 2 e 3 do NFS n√£o trabalham com *estado*, assim o servidor n√£o controla quais clientes montaram um determinado compartilhamento. Para contornar isso, o servidor NFS envia um cookie na conclus√£o de uma montagem bem-sucedida. Esse cookie identifica o diret√≥rio montado para o servidor NFS, dessa forma temos uma maneira para o cliente acessar seu conte√∫do. Os cookies persistem entre as reinicializa√ß√µes do servidor, portanto, uma falha n√£o deixa com que o cliente fique num estado irrecuper√°vel. O cliente pode simplesmente esperar at√© que o servidor esteja dispon√≠vel novamente e reenviar a solicita√ß√£o.
J√° na vers√£o 4 do NFS, o mesmo passou a ser um protocolo com monitora√ß√£o de estado (tanto o cliente quanto o servidor mant√™m informa√ß√µes sobre arquivos abertos e bloqueios). Se uma falha acontecer no servidor, os clientes v√£o auxiliar o servidor no processo de recupera√ß√£o, enviando ao servidor suas informa√ß√µes de estado antes da falha ocorrer. Um servidor de retorno espera por um per√≠odo predefinido para que os clientes anteriores relatem suas informa√ß√µes de estado antes de permitir novas opera√ß√µes e bloqueios. 

O uso de cookies n√£o existe mais na vers√£o 4 do NFS.



## Exports - Mudan√ßa da vers√£o

Nas vers√µes 2 e 3 do NFS, cada exporta√ß√£o √© tratada como uma exporta√ß√£o independente, √© exportada separadamente independente do diret√≥rio (exportar o **/var/log/** significa que voce vai exportar apenas ele e nada mais; tudo o que tiver dentro dele tamb√©m). Na vers√£o 4, um servidor exporta um √∫nico *Pseudo-File_System* que incorpora todos os seus diret√≥rios exportados.

Em resumo, o Pseudo-File_System √© um namespace do sistema de arquivos do servidor que remove qualquer coisa dentro do diret√≥rio raiz do Pseudo-File_System (sim, o Pseudo-File_System possui um diret√≥rio raiz) que n√£o tenha sido previamente exportado.

| Diret√≥rios      | Exportado? |
| --------------- | ---------- |
| **/var/log/**   | Sim        |
| /var/backups/   | N√£o        |
| /var/crash/     | N√£o        |
| **/var/cache/** | Sim        |
| /var/lib/       | N√£o        |
| **/etc**        | Sim        |
| /var/lock/      | N√£o        |
| **/var/www/**   | Sim        |

No NFS vers√£o 3, cada diret√≥rio exportado deve ser configurado separadamente no cliente, no nosso caso, teriamos que montar 3 diret√≥rios. No NFS vers√£o 4, o pseudo-filesystem agrupa todo os diret√≥rios no diret√≥rio raiz do pseudo-filesystem. Esse pseudo-filesystem segue por padr√£o a estrutura do FHS, ao inv√©s vez de solicitarmos v√°rias montagens separada para cada um dos diret√≥rios **/var/www**, **/var/cache**, **/var/log** e **/etc**, o cliente agora pode montar todos os diret√≥rios usando o pseudo-filesystem-raiz do servidor e navegar na hierarquia semelhante ao FHS.

Vamos ao exemplo pr√°tico:

- Exports feitos no servidor

  ```bash
  root@ServerNFS:~\# exportfs
  /var/log      	192.168.122.0/24
  /var/cache    	192.168.122.0/24
  /var/www      	192.168.122.0/24
  /etc          	192.168.122.0/24
  ```

- J√° no cliente, podemos montar apenas o diret√≥rios raiz:

  ```bash
  # Montando a raiz "/" do pseudo-filesystem:
  $ sudo mount 192.168.122.137:/ /mnt
  
  # Agora verifique o que temos la dentro:
  $ ls /mnt/
  etc  var
  
  $ ls /mnt/var/
  cache  log  www
  ```

  Como pode ver, ele tem apenas os diret√≥rios que foram exportados pelo servidor, mesmo que tenhamos montado o **/** do servidor (pseudo-filesystem). Para mudar a raiz do pseudo-filesystem, use a op√ß√£o **fsid=0** no compartilhamento que deseja tornar a raiz.

De acordo com Nemeth, Snyder, Hein Trent, Whaley e Mackin (2017, p. 797).

> Os clientes NFSv4 puros n√£o podem examinar a lista de montagens em um servidor remoto. Em vez disso, eles simplesmente montam a pseudo-raiz e, em seguida, todas as exporta√ß√µes dispon√≠veis tornam-se acess√≠veis por meio desse ponto de montagem.
>
> Essa √© a hist√≥ria de acordo com as especifica√ß√µes RFC. Na pr√°tica, a situa√ß√£o √© um tanto confusa. A implementa√ß√£o do Solaris est√° em conformidade com esta especifica√ß√£o. O Linux fez uma tentativa indiferente de oferecer suporte ao pseudo-sistema de arquivos no c√≥digo NFSv4 anterior, mas posteriormente o revisou para suportar o esquema de forma mais completa; a vers√£o de hoje parece respeitar a inten√ß√£o da RFC. 
>
> O FreeBSD n√£o implementa o pseudo-sistema de arquivos conforme descrito pela RFC. A sem√¢ntica de exporta√ß√£o do FreeBSD √© essencialmente a mesma da vers√£o 3; todos os subdiret√≥rios em uma exporta√ß√£o est√£o dispon√≠veis para os clientes.



## Seguran√ßa

O protocolo NFS originalmente foi projetado sem nenhuma preocupa√ß√£o com a seguran√ßa dos exports.

A vers√£o 4 do NFS abordou quest√µes de seguran√ßa de vers√µes anteriores, todas as vers√µes do protocolo NFS s√£o independentes do mecanismo de seguran√ßa, alguns dos m√©todos de seguran√ßa incluem:

| Mecanismo  | Descri√ß√£o                                                    |
| ---------- | ------------------------------------------------------------ |
| AUTH_NONE  | Sem autentica√ß√£o.                                            |
| AUTH_SYS   | Controle de acesso de usu√°rio e grupo no estilo UNIX (Padr√£o para muitos ambientes). |
| RPCSEC_GSS | Utiliza uma autentica√ß√£o centralizada como LDAP e Kerbero.   |

O m√©todo de seguran√ßa mais comum usado √© o AUTH_SYS, tanto no passado como hoje em dia, ele depende dos identificadores de usu√°rio e grupo UNIX (UID e GID). Aqui, o cliente envia o UID e o GID local do usu√°rio que est√° solicitando acesso ao arquivo/diret√≥rio, o servidor ent√£o vai comparar os valores com aqueles de seu pr√≥prio arquivo **/etc/passwd** e determina se o usu√°rio deve ter acesso. 

> Se os usu√°rios *jo√£o* e *rodrigo* tiverem o mesmo UID em dois clientes diferentes (joao tem uid 1003 no servidor e rodrigo tem uid 1003 no cliente), eles ter√£o acesso aos arquivos um do outro.
>
> Esse problema acontece mais em ambientes onde a Home do usu√°rio √© compartilhada via rede.
>
> Al√©m disso, os usu√°rios que possuem acesso root em um sistema podem usar o comando *su* para obter qualquer UID que desejarem; o servidor ent√£o dar√° a eles acesso aos arquivos do usu√°rio correspondente.
>
> S√≥ que n√£o, uma t√©cnica foi implementada por poadr√£o, mas isso pode ser quebrado.

Para evitar todos esses problemas, a maioria dos servidores podem usar um m√©todo de seguran√ßa mais forte, como o Kerberos, em combina√ß√£o com o RPCSEC_GSS. Essa configura√ß√£o requer que o cliente e o servidor participem de um dom√≠nio Kerberos. Como o Kerberos (Assim como LDAP) autentica os usu√°rios centralmente, dessa forma evitamos os problemas de auto-identifica√ß√£o descritos acima. O Kerberos tamb√©m pode fornecer criptografia forte e integridade garantida para arquivos transferidos pela rede. Todos os sistemas NFS vers√£o 4 em conformidade com o protocolo devem implementar RPCSEC_GSS, mas √© opcional na vers√£o 3.



## Desempenho na nova vers√£o 

O NFS vers√£o 4 foi projetado para ter seu desempenho melhorado em compara√ß√£o com a vers√£o 3:

- Na vers√£o 3 temos uma s√©rie de requisi√ß√µes para rpcbind (antigo portmap), para o mountd e tamb√©m para o NFS, isso aumenta a quantidade de pacotes na rede sem falar na dependencia de v√°rios protocolos funcionando separadamente.

- Na vers√£o 4 foi introduzido um RPC chamado COMPOUND que agrupa v√°rias opera√ß√µes em uma solicita√ß√£o, reduzindo a sobrecarga e a lat√™ncia decorrentes de v√°rias chamadas de procedimento remoto (como acontecia na vers√£o 3), apesar de n√£o utilizar rpcbind e mountd como na vers√£o 3, ele ainda √© usado na vers√£o 4, acontece que agora n√£o vemos mais os pacotes de comunica√ß√£o ao fazer a captura de pacotes na rede. Mas se voc√™ desabilitar os daemons do *rpcbind* e  *nfs-mountd*, ver√° que n√£o ser√° poss√≠vel montar nenhum diret√≥rio, tudo indicar que o uso desses RPCs s√£o internos do NFS, mas eu n√£o posso afimar, a √∫nica coisa que posso dizer √© que NFS vers√£o 4 n√£o funciona sem eles estarem ativos.

Esses recursos descrito acima √© o funcionamento do protocolo NFS nas camadas mais baixas, por isso n√£o requerem muita aten√ß√£o dos administradores do sistema, j√° que n√£o temos muito como atuar nessa parte, mas √© bom conhecer.



## Funcionamento do NFS

O servidor NFS "exporta" o diret√≥rio para a m√°quina cliente, isso acontece assim que o cliente monta o diret√≥rio remoto como se fosse um diret√≥rio local. 

Na NFS vers√£o 3, o processo usado pelos clientes para montar um sistema de arquivos √© separado do processo usado para acessar os arquivos. 

Primeiro o cliente faz um triple-way handshake (aperto de m√£os) com o servidor (j√° que ele usa TCP para comunica√ß√£o com o protocolo NFS).

![Selection_145](IMG/Selection_145.png)

Ap√≥s estabelecido comunica√ß√£o entre cliente e servidor (IP do servidor=192.168.10.10 e IP do cliente=192.168.10.180), o cliente vai enviar uma requisi√ß√£o GETPORT na porta 111 (novo rpcbind, antigo portmap), solicitando a porta usada pelo NFS:

![Selection_133](IMG/Selection_133.png)

E o servidor responde com a porta usada pelo NFS, no nosso caso, a porta 2049 (porta padr√£o):

![Selection_134](IMG/Selection_134.png)

> As comunica√ß√µes com a porta 2049 s√£o feitas preferencialmente via TCP na vers√£o 3, na vers√£o 4 √© somente TCP.
>
> 
>
> Foquemos na vers√£o 3 agora.
>
> As comunica√ß√µes na porta 111 (*rpcbind*) v√£o depender do tipo de consulta do cliente.
>
> Caso o cliente n√£o escolha um protocolo de transporte, a consulta que √© realizada no *rpcbind* (GETPORT) para descobrir a porta do NFS √© TCP (n√£o entendi o  motivo, voc√™ ver√° o porque). 
> As mesmas consultas GETPORT que s√£o feitas no *rpcbind* (feitas anteriormente em TCP para NFS) agora s√£o feita usando UDP para descobrir a porta usada pelo *mountd*... 
> Consequentemente, as consultas realizada diretamente a *mountd* tamb√©m s√£o UDP (f√°cil de entender o porque).
>
> Caso o cliente escolha o protocolo de transporte como sendo TCP, todas as requisi√ß√µes para *rpcbind* e *mountd* v√£o ser feitas somente em TCP e caso tenha escolhido UDP, tudo ser√° feito com UDP (funcionamento esperado).
>
> Isso deixa claro uma coisa, o cliente tem o poder de escolher qual protocolo de transporte ser√° usado (isso se estiver habilitado no servidor).
>
> 
>
> Resumo:
>
> Preferencialmente o protocolo (*rpcbind*) escolhe comunica√ß√µes UDP, n√£o sendo poss√≠vel alterar somente para TCP no daemon do *rpcbind*, mas o cliente pode alterar esse comportamento, portanto, voc√™ pode notar requisi√ß√µes para *rpcbind* (porta 111) sendo UDP ou TCP, fique ciente que isso depende do cliente escolher usar TCP, se n√£o escolher, ser√° usado UDP (s√≥ n√£o ser√° usado UDP na comunica√ß√£o para saber a porta do NFS).

Depois disso, uma s√©rie de comunica√ß√µes √© feita entre cliente e servidor atrav√©s do *rpcbind*. Agora nosso cliente envia uma requisi√ß√£o para o servidor na porta 111 (rpcbind) solicitando a porta que est√° sendo usada pelo mountd (tamb√©m conhecido como *rpc.mountd*):

> *mountd* (rpc.mountd) roda em portas aleatorias por padr√£o (isso pode ser alterado), √© responsabilidade do *rpcbind* dizer ao cliente qual porta deve ser usada para comunica√ß√£o com *mountd*. 
>
> A fun√ß√£o do *mountd* √© fazer a montagem dos diretorios remotos.

![Selection_135](IMG/Selection_135.png)

Ap√≥s o servidor receber a consulta, ele responde com a porta usada pelo *mountd* (porta 56781 no nosso caso):

![Selection_136](IMG/Selection_136.png)

Agora uma s√©rie de comunica√ß√µes ser√° feita com *rpcbind* novamente, e depois disso, uma comunica√ß√£o com *mountd* √© realizada, nessa comunica√ß√£o o cliente informa qual diret√≥rio gostaria de montar em seu sistema:

![Selection_137](IMG/Selection_137.png)

> O cliente quer montar o diret√≥rio /srv/distros/ubuntu2004

Se tudo der certo, o servidor responde positivamente (**Status: OK (0)**):

![Selection_138](IMG/Selection_138.png)

Depois disso outra s√©rie de comunica√ß√µes √© feita entre cliente e servidor para finalizar a montagem do diret√≥rio, a comunica√ß√£o final tem andamento na porta 2049 (protocolo do nfs, tamb√©m conhecido como NFSD), aqui voc√™ j√° deve ter o diretorio pronto para uso.

Na vers√£o 4 as exporta√ß√µes s√£o apresentadas aos clientes como uma √∫nica hierarquia de sistema de arquivos por meio do pseudo-filesystem, j√° explicado em **Exports - Mudan√ßa da vers√£o 3 para 4**. Com sobre o funcionamento da comunica√ß√£o na vers√£o 4 n√£o tem muito segredo, o diferencial √© que o uso do *rpcbind* e *mountd* n√£o s√£o mais necess√°rios, o cliente abre apenas 1 comunica√ß√£o com o servidor usando a chamada COMPOUND que far√° tudo que for necess√°rio para montar o filesystem do NFS, essas chamadas s√£o realizadas na porta 2049 (onde roda o daemon do NFSD).



### Comportamento notado

Existem algumas observa√ß√µes que eu pude notar com o uso do NFS, ainda mais quando eu tive a incr√≠vel ideia de for√ßar ele a usar somente a vers√£o 4, isso ainda vai ser falado mais para frente, mas quis dedicar esse espa√ßo para essas observa√ß√µes.

Por padr√£o tanto cliente como servidor v√£o usar o NFS vers√£o 4, desde que algumas coisas n√£o aconte√ßam.

Se voc√™ mudar a porta do NFS (padr√£o 2049), o cliente vai tentar comunica√ß√£o nessa porta e ent√£o vai falhar, com isso ele faz um GETPORT no rpcbind diretamente, aqui ja vemos que come√ßou a usar NFS vers√£o 3.

Eu tentei inclusive fazer um redirecionamento de porta, para tentar enganar o sistema, mas por algum motivo ele sabe e passar a usar o rpcbind, com isso, ele acaba usando NFS vers√£o 3 (talvez eu n√£o tenha feito direito).

Se voce desativar rpcbind e nfs-mountd, o NFS n√£o vai funcioanar, independente da vers√£o, muitos publicadores de conte√∫do dizem que voc√™ deve desativar vers√µes anteriores do NFS se n√£o for usar, e ficar preferencialmente com NFS vers√£o 4, mas o que eu pude notar √© que ele usa internamente RPC ainda, mesmo que isso n√£o fique vis√≠vel para n√≥s, digo isso porque ap√≥s desativar rpcbind e nfs-mountd e for√ßar o NFS a usar a vers√£o 4, ele funcionar√°.



## Arquivo exports

O NFS usa um √∫nico banco de dados de controle de acesso que informa quais diret√≥rios devem ser exportados e quem podem mont√°-los.

O arquivo respons√°vel por manter os dados de acesso (diret√≥rios a serem exportados e quem pode acessar) fica em **/etc/exports**. Esse arquivo √© lido pelo comando `exportfs -a` no Linux e vai ser lido ao reiniciar o servi√ßo do NFS no FreeBSD. 

Ao editar **/etc/exports**, execute `exportfs -ra` para ativar suas mudan√ßas no Linux ou execute `service nfsd restart` no FreeBSD. Segue uma tabela contendo os comandos e suas descri√ß√µes.

| Comando                | Sistema       | Descri√ß√£o                                                    |
| ---------------------- | ------------- | ------------------------------------------------------------ |
| exportfs -ra           | Linux         | Re-exporta todos os diret√≥rios no arquivo.                   |
| service mountd reload  | FreeBSD/Linux | Reinicia o daemon do mountd.                                 |
| service rpcbind reload | FreeBSD/Linux | Reinicia o daemon do rpcbind.                                |
| service nfsd reload    | FreeBSD/Linux | Reinicia o daemon do rpcbind e faz a re-leitura do arquivo de exporta√ß√£o. |

Em ambientes Linux com Systemd √© recomendado o uso  `systemctl restart servi√ßo`.

O arquivo de exporta√ß√£o consiste em uma lista de diret√≥rios exportados seguido pelos hosts que t√™m permiss√£o para acess√°-los e op√ß√µes associadas ao ponto de montagem/funcionamento do NFS. Um espa√ßo em branco deve separar o diret√≥rio a ser exportado da lista de clientes, podemos ter mais de um cliente liberado para acesso, cada cliente √© seguido imediatamente por uma lista de op√ß√µes separadas por v√≠rgulas entre par√™nteses.

Por exemplo, estou exportando o `/home`, para todos que est√£o na rede `192.168.122.0/24`:

`/home 192.168.122.0/24(ro)`

Nesse caso, eles s√≥ ter√£o acesso a leitura `ro (Read-Only)`. Podemos usar o asterisco `*` onde deveriamos colocar o IP para especificar qualquer destino: `/home *(ro)`.



## Daemons relacionados ao NFS

Vou apresentar uma breve descri√ß√£o sobre os daemons usados na comunica√ß√£o do NFS, o foco aqui √© mais para a vers√£o 3, j√° que a vers√£o 4 s√≥ temos o uso do `nfsd`.



### RPCBIND

O funcionamento desse daemon foi explicado em **Funcionamento do NFS**, mas numa r√°pida introdu√ß√£o.
Antigamente era chamado de `portmap`, ainda hoje √© poss√≠vel ver logs e registros usando esse nome, como o Wireshark, roda na porta 111, ele server para informar ao cliente NFS a porta que o protocolo /daemon do NFS/mountd est√£o usando, dessa forma, o cliente sabe em qual porta se conectar.

Seu arquivo de configura√ß√£o fica em `/etc/default/rpcbind`, mas pode ser usado um outro arquivo, localizado em `/etc/rppcbind.conf` (normalmente esse segundo arquivo n√£o existe, tendo somente o primeiro arquivo).

O script que inicia o *rpcbind*, que √© executado pelo seu daemon fica em `/sbin/rpcbind`. N√£o existem muitas op√ß√µes flex√≠veis para trabalharmos, mas d√™ uma olhada voc√™ mesmo, pode achar algo interessante, rode `man rpcbind`.



### Mountd

No Linux o daemon do *mountd* chama-se `nfs-mountd.service`, *mountd* n√£o possui um arquivo de configura√ß√£o, ele usa op√ß√µes passadas como argumento ao script, essas op√ß√µes podem ser colocadas num arquivo que foi criado com esse prop√≥sito. Ao reiniciar o daemon do *nfsd*, as configura√ß√µes do *mountd* tamb√©m s√£o carregadas, isso porque no daemon do *nfsd* est√° configurado para ter esse comportamento, por isso, muitas vezes s√≥ precisamos reiniciar o *nfsd* (mountd acabar√° sendo reiniciado tamb√©m).

Seu script fica em `/usr/sbin/rpc.mountd`. Como mountd roda em portas aleat√≥rias, isso cria uma dificuldade para gerenciar todas as portas no firewall, por isso vamos ver como alterar/fixar sempre a mesma porta.
Acesse o arquivo `/etc/default/nfs-kernel-server`, nesse arquivo vai ter uma vari√°vel usada pelo daemon do *mountd*, chamada `RPCMOUNTDOPTS`, para fixar uma porta use a op√ß√£o `--port <porta>`, no final dessa se√ß√£o, vou mostrar um arquivo b√°sico padr√£o que voc√™ deve ter para facilitar a implementa√ß√£o com firewall.



### NFSD

No Linux o *nfsd* n√£o tem arquivo de configura√ß√£o, assim como *mountd*, usa op√ß√µes passadas como argumentos de linha de comando, inclusive, utilizam o mesmo arquivo `/etc/default/nfs-kernel-server`. 

Por padr√£o o *nfsd* recebe um argumento num√©rico `8`, isso especifica quantos threads de servidor bifurcar. Selecionar o n√∫mero apropriado de *threads nfsd* √© muito importante, se o n√∫mero for muito baixo ou muito alto, o desempenho do NFS pode ser prejudicado.

O n√∫mero ideal das threads vai depender do S.O. que vamos usar bem como do hardware. Se voc√™ notar que o comando `ps` geralmente mostra o *nfsd* no estado **D** (ininterrupto hiberna√ß√£o) e que alguma CPU ociosa est√° dispon√≠vel, considere aumentar o n√∫mero de threads. 
Se voc√™ encontrar a m√©dia de carga (conforme relatado pelo tempo de atividade) aumentando √† medida que adiciona mais threads, voc√™ passou do limite, ent√£o deve diminuir a quantidade de threads.

Execute o **nfsstat** regularmente para verificar os problemas de desempenho que podem estar associados ao n√∫mero de threads do *nfsd*.

No FreeBSD, as op√ß√µes **--minthreads** e **--maxthreads** permitem especificar o n√∫mero de threads, usando um limite m√≠nimo e m√°ximo.

A vari√°vel usada pelo *nfsd* √© chamada `RPCNFSDARGS`, no arquivo onde configuramos esses parametros (para deixar permanente) ela tem o nome de `RPCNFSDCOUNT`, tudo o que estiver nessa vari√°vel ser√° importada para a vari√°vel `RPCNFSDARGS`. Seu script fica em `/usr/sbin/rpc.nfsd` e seu daemon chama-se `nfs-kernel-server` e tem um alias chamado `nfs-server` (para facilitar as coisas üôÇ).

Em ambientes RedHat o arquivo `nfs-kernel-server` fica em `/etc/sysconfig/nfs`.



## Acesso root e nobody

Essa se√ß√£o se aplica a ambientes que usem *AUTH_SYS* preferencialmente. Os usu√°rios devem sempre receber privil√©gios id√™nticos onde quer que v√£o o mesmo usu√°rio em duas m√°quinas diferentes devem ter o mesmo acesso (normalmente, voc√™ j√° vai entender),  √© importante evitar que o root seja executado excessivamente em sistemas de arquivos montados em NFS, isso porque um usu√°rio com root no sistema remoto poderia ter acesso as arquivos de outros usu√°rios, do qual ele n√£o deveria ter esse acesso.

Um exemplo disso √© quando usamos a home dos usu√°rios montados via rede, √© importante evitar que o root possa acessar o conte√∫do da home dos usu√°rios do NFS. Por padr√£o, o servidor NFS intercepta as solicita√ß√µes feitas que tenham o UID 0 e as altera para que pare√ßam ter vindo de outro usu√°rio, essa t√©cnica √© chamada de "squashing root".

Uma conta reservada chamada "nobody" √© definida especificamente para ser o pseudo-usu√°rio, ou seja, o usu√°rio root ao tentar acesso √© mapeado para o usu√°rio *nobody*, que possui o UID 65.534. 

√â poss√≠vel alterar os mapeamentos UID e GID padr√£o para root no arquivo de exporta√ß√£o. Alguns sistemas t√™m uma op√ß√£o all_squash para mapear todos os UIDs do cliente para o mesmo UID de pseudo-usu√°rio no servidor, isso significa que todos os usu√°rio v√£o ser interpretados sendo o usu√°rio *nobody*, muito recomendado para ambientes publicos.

Que fique claro, esse m√©todo √© usado para impedir que o root acesse arquivos de outros usu√°rio, apenas isso, pode parecer pouco ou insignificante para a maioria dos ambientes que usam NFS, mas esse t√©cnica √© muito importante. O √∫nico efeito aceit√°vel da destrui√ß√£o do squashing root √© permitir o acesso do root aos arquivos que pertencem ao root, fazendo isso voc√™ devolve o poder do root para ele, cuidado com isso, se voc√™ for mesmo root, provavelmente ter√° acesso ao servidor, ent√£o acho d√≠ficil uma justificativa para destruir squashing root.

Para desativar *root_swash*, defina a op√ß√£o **no_root_squash**. 



Op√ß√µes gerais usadas no exports:

| Op√ß√£o                | Descri√ß√£o                                                    |
| -------------------- | ------------------------------------------------------------ |
| secure               | Esta op√ß√£o requer que as solicita√ß√µes sejam originadas em uma porta da Internet menor que IPPORT_RESERVED (1024). <br />Essa op√ß√£o est√° ativada por padr√£o. Para deslig√°-lo, especifique Insecure.<br />Ao capturar pacotes de NFS, voc√™ pode verificar que as requisi√ß√µes do cliente sempre saem de portas abaixo de 1024, tornando essa regra verdadeira, na Internet, essas portas tem usos especificos. |
| rw                   | Permite leitura e grava√ß√£o em volumes NFS. O padr√£o √© proibir qualquer solicita√ß√£o que mude o sistema de arquivos, igual a  **ro**. |
| async                | Esta op√ß√£o permite que o servidor NFS viole o protocolo NFS e responda √†s solicita√ß√µes antes que quaisquer altera√ß√µes feitas por essa solicita√ß√£o sejam confirmadas para armazenamento est√°vel (por exemplo, unidade de disco).<br/><br/>Usar essa op√ß√£o geralmente melhora o desempenho, mas ao custo de uma reinicializa√ß√£o do servidor impura (ou seja, uma falha) pode fazer com que os dados sejam perdidos ou corrompidos. |
| sync                 | Responda √†s solicita√ß√µes somente depois que as altera√ß√µes forem confirmadas para armazenamento est√°vel.<br/><br/>Em vers√µes de nfs-utils at√© e incluindo 1.0.0, a op√ß√£o async era o padr√£o. Em todas as vers√µes posteriores a 1.0.0, a sincroniza√ß√£o √© o padr√£o e async deve ser solicitado explicitamente, se necess√°rio. <br/><br/>Para ajudar a tornar os administradores do sistema cientes desta mudan√ßa, exportfs ir√° emitir um aviso se nem sync nem async forem especificados.<br /><br />O uso dessa op√ß√£o teoricamente, piora o desempenho, nada que seja not√°vel nos dias de hoje, gra√ßas ao poder dos hardwares atuais, mas com essa op√ß√£o, temos uma maior garantia e confiabildiade dos dados. |
| no_subtree_check     |                                                              |
| fsid=num\|root\|uuid | O NFS precisa ser capaz de identificar cada sistema de arquivos que exporta.  Como nem todos os sistemas de arquivos s√£o armazenados em dispositivos, e nem todos os sistemas de arquivos t√™m UUIDs, √†s vezes √© necess√°rio dizer explicitamente ao NFS como identificar um sistema de arquivos.<br />Para NFSv4, h√° um sistema de arquivos distinto que √© a raiz de todos os sistemas de arquivos exportados. Isso √© especificado com `fsid=root` ou `fsid=0`, ambos significando exatamente a mesma coisa. |
| root_squash          | Mapeie as solicita√ß√µes de uid/gid 0 para o uid/gid an√¥nimo.<br /><br />Observe que isso n√£o se aplica a quaisquer outros uids ou gids que possam ser igualmente sens√≠veis, como compartimento de usu√°rio ou equipe de grupo. |
| no_root_squash       | Desligue o squashing root.                                   |
| all_squash           | Mapeie todos os uids e gids para o usu√°rio an√¥nimo/nobody. √ötil para diret√≥rios FTP p√∫blicos exportados por NFS, diret√≥rios de spool de not√≠cias, etc. A op√ß√£o oposta √© no_all_squash, que √© a configura√ß√£o padr√£o usada na op√ß√£o *sec*. |
| anonuid e anongid    | Essas op√ß√µes definem explicitamente o uid e o gid da conta an√¥nima. Esta op√ß√£o √© √∫til principalmente para clientes PC/NFS, onde voc√™ pode desejar que todas as solicita√ß√µes pare√ßam ser de um usu√°rio. |

**no_subtree_check**

Cemo a descri√ß√£o dessa op√ß√£o √© grande no manual, ela n√£o cabe na tabela acima, ent√£o vou deixar a explica√ß√£o abaixo:

> Esta op√ß√£o desativa a verifica√ß√£o de sub√°rvore, o que tem implica√ß√µes de seguran√ßa leves, mas pode melhorar a confiabilidade em algumas circunst√¢ncias.
>
> Se um subdiret√≥rio de um sistema de arquivos √© exportado, mas todo o sistema de arquivos n√£o √©, sempre que uma solicita√ß√£o NFS chega, o servidor deve verificar n√£o apenas se o arquivo acessado est√° no sistema de arquivos apropriado (o que √© f√°cil), mas tamb√©m se est√° no √°rvore exportada (o que √© mais dif√≠cil). Essa verifica√ß√£o √© chamada de subtree_check.
>
> Para realizar esta verifica√ß√£o, o servidor deve incluir algumas informa√ß√µes sobre a localiza√ß√£o do arquivo no "identificador de arquivos" que √© fornecido ao cliente. Isso pode causar problemas ao acessar arquivos que s√£o renomeados enquanto um cliente os mant√©m abertos (embora em muitos casos simples ainda funcione).
>
> A verifica√ß√£o de sub√°rvore tamb√©m √© usada para certificar-se de que os arquivos dentro de diret√≥rios aos quais apenas o root tem acesso podem ser acessados apenas se o sistema de arquivos for exportado com no_root_squash, mesmo se o pr√≥prio arquivo permitir um acesso mais geral.
>
> Como um guia geral, um sistema de arquivos de diret√≥rio pessoal, que normalmente √© exportado na raiz e pode ter muitas renomea√ß√µes de arquivos, deve ser exportado com a verifica√ß√£o de sub√°rvore desabilitada. Um sistema de arquivos que √© principalmente somente leitura, e pelo menos n√£o v√™ muitos arquivos renomeados (por exemplo, /usr ou /var) e para o qual subdiret√≥rios podem ser exportados, provavelmente deve ser exportado com verifica√ß√µes de sub√°rvore habilitadas.
>
> A partir da vers√£o 1.1.0 do nfs-utils, o padr√£o ser√° no_subtree_check j√° que subtree_checking tende a causar mais problemas do que vale a pena. Se voc√™ realmente precisa da verifica√ß√£o de sub√°rvore, deve colocar explicitamente essa op√ß√£o no arquivo de exporta√ß√£o. Se voc√™ n√£o colocar nenhuma op√ß√£o, exportfs ir√° avis√°-lo de que a mudan√ßa est√° pendente.



## Resumo

Agora que j√° conhecemos bem o funcionamento do NFS (embora o conte√∫do desse documento n√£o aborde m√©todos avan√ßados de uso dele), temos uma base s√≥lida para usar tal protocolo. 

Sempre que quisermos exportar algum diret√≥rio, o mesmo deve estar em `/etc/exports`, exemplo:

```bash
/var/log/ 192.168.122.0/24(rw,sync,no_subtree_check)
# Aqui estou exportando o /var/log.
```



Para configurarmos portas espec√≠ficas a serem usadas por *mountd* (lembre que ele usa v√°rias portas aleat√≥rias, dificultando o uso com firewalls), a porta deve ser fixada no arquivo `/etc/default/nfs-kernel-server` para ambientes Debian e `/etc/sysconfig/nfs` para Redhat.

Segue um exemplo de meu arquivo:

```bash
# Number of servers to start up
RPCNFSDCOUNT="8"

# Runtime priority of server (see nice(1))
RPCNFSDPRIORITY=0

# Options for rpc.mountd.
# If you have a port-based firewall, you might want to set up
# a fixed port here using the --port option. For more information, 
# see rpc.mountd(8) or http://wiki.debian.org/SecuringNFS
# To disable NFSv4 on the server, specify '--no-nfs-version 4' here
RPCMOUNTDOPTS="--manage-gids --port 37240"

# Do you want to start the svcgssd daemon? It is only required for Kerberos
# exports. Valid alternatives are "yes" and "no"; the default is "no".
NEED_SVCGSSD=""

# Options for rpc.svcgssd.
RPCSVCGSSDOPTS=""
```

Lembre-se que para configurar qualquer coisa para o NFS em s√≠ (nfsd) usamos a vari√°vel `RPCNFSDCOUNT`, que nesse caso possui o n√∫mero de threads que o servidor vai usar.

E para configurar qualquer coisa para *mountd*, usamos a vari√°vel `RPCMOUNTDOPTS`, que nesse caso, estamos informando a porta que ele ir√° usar, `manage-gids` faz com que sejam aceitados as solicita√ß√µes do kernel para mapear n√∫meros de id de usu√°rio em listas de n√∫meros de id de grupo para uso no controle de acesso.



## Comandos

Vamos ver alguns comandos que v√£o nos ajudar no dia a dia.



### exportfs

Usado para fazer manuten√ß√£o na tabela de filesystems exportados do NFS (edita os diret√≥rios exportados).

As op√ß√µes mais usadas s√£o:

| Op√ß√£o       | Descri√ß√£o                                                    |
| ----------- | ------------------------------------------------------------ |
| -d          | Ativa o Debug                                                |
| -a          | Esporta tudo o que estiver no arquivo de exports.            |
| -o <op√ß√µes> | Informa as op√ß√µes que s√£o fornecidas no arquivo de exports.  |
| -r          | Re-exporta tudo.                                             |
| -u          | Se usado sozinho n√£o surte efeito, precisa ser usado com a op√ß√£o `-a` (all) ou informar o diret√≥rio que gostaria de parar de exportar. |
| -v          | Ativa o verbose quando for exportar ou defazer o exporte.    |
| -s          | Exibe os diret√≥rios que foram exportados, com detalhes das op√ß√µes usadas. |

Embora possamos exportar os diret√≥rios no exports, tamb√©m podemos exportar um diret√≥rio temporariamente, para isso usamos a sintaxe `exportfs IP:folder -o options`, veja um exemplo:

```bash
exportfs 192.168.0.0/24:/var/log -o ro,sync,no_subtree_check
```



### showmount

Usado para verificar os diret√≥rios que foram exportados, `exportfs` j√° faz isso, mas possui op√ß√µes que n√£o tem nele.

| Op√ß√£o | Descri√ß√£o                                                    |
| ----- | ------------------------------------------------------------ |
| -e    | Mostra os diret√≥rios exportados.                             |
| -a    | Mostra quem e qual diret√≥rio foi montado, vai mostrar o hostname ou IP do cliente seguido do diret√≥rio que ele montou. |
| -d    | Mostra somente o diret√≥rio exportado que j√° foi montado por um cliente, mas s√≥ mostra o nome do diret√≥rio. |



### rpcinfo

Exibe informa√ß√µes do RPC.

A op√ß√£o mais usada √© a `-p`, onde exibe um resumo das vers√µes ativas do NFS, mountd e rpcbind, quais portas eles est√£o usando e quais protocolos de transportes est√£o ativos.

```bash
root@NFServer:~# rpcinfo -p
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
    100005    1   udp   5600  mountd
    100005    1   tcp   5600  mountd
    100005    2   udp   5600  mountd
    100005    2   tcp   5600  mountd
    100005    3   udp   5600  mountd
    100005    3   tcp   5600  mountd
    100003    3   tcp   7000  nfs
    100003    4   tcp   7000  nfs
    100227    3   tcp   7000
    100003    3   udp   7000  nfs
    100227    3   udp   7000
    100021    1   udp  43950  nlockmgr
    100021    3   udp  43950  nlockmgr
    100021    4   udp  43950  nlockmgr
    100021    1   tcp  33751  nlockmgr
    100021    3   tcp  33751  nlockmgr
    100021    4   tcp  33751  nlockmgr
```



### NFSSTAT

Mostra estat√≠stica do NFS, √© um comando bem complexo de se entender, possu√≠ muita informa√ß√µes em sua sa√≠da, o recomendado √© que se entenda sobre cada op√ß√£o exibida (n√£o vou focar nisso aqui).



## Melhorando o desempenho

